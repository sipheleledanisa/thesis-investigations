{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import jdc\n",
    "import tensorflow as tf\n",
    "from distutils.version import StrictVersion\n",
    "\n",
    "from maci.misc import logger\n",
    "from maci.misc.overrides import overrides\n",
    "from maci.misc import tf_utils\n",
    "\n",
    "from maci.learners.base import MARLAlgorithm\n",
    "\n",
    "EPS = 1e-6"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/siphelele/anaconda3/envs/masql/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/siphelele/anaconda3/envs/masql/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/siphelele/anaconda3/envs/masql/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/siphelele/anaconda3/envs/masql/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/siphelele/anaconda3/envs/masql/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/siphelele/anaconda3/envs/masql/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def adaptive_isotropic_gaussian_kernel(xs, ys, h_min=1e-3):\n",
    "    \"\"\"Gaussian kernel with dynamic bandwidth.\n",
    "    The bandwidth is adjusted dynamically to match median_distance / log(Kx).\n",
    "    See [2] for more information.\n",
    "    Args:\n",
    "        xs(`tf.Tensor`): A tensor of shape (N x Kx x D) containing N sets of Kx\n",
    "            particles of dimension D. This is the first kernel argument.\n",
    "        ys(`tf.Tensor`): A tensor of shape (N x Ky x D) containing N sets of Kx\n",
    "            particles of dimension D. This is the second kernel argument.\n",
    "        h_min(`float`): Minimum bandwidth.\n",
    "    Returns:\n",
    "        `dict`: Returned dictionary has two fields:\n",
    "            'output': A `tf.Tensor` object of shape (N x Kx x Ky) representing\n",
    "                the kernel matrix for inputs `xs` and `ys`.\n",
    "            'gradient': A 'tf.Tensor` object of shape (N x Kx x Ky x D)\n",
    "                representing the gradient of the kernel with respect to `xs`.\n",
    "    Reference:\n",
    "        [2] Qiang Liu,Dilin Wang, \"Stein Variational Gradient Descent: A General\n",
    "            Purpose Bayesian Inference Algorithm,\" Neural Information Processing\n",
    "            Systems (NIPS), 2016.\n",
    "    \"\"\"\n",
    "    Kx, D = xs.get_shape().as_list()[-2:]\n",
    "    Ky, D2 = ys.get_shape().as_list()[-2:]\n",
    "    assert D == D2\n",
    "\n",
    "    leading_shape = tf.shape(xs)[:-2]\n",
    "\n",
    "    # Compute the pairwise distances of left and right particles.\n",
    "    diff = tf.expand_dims(xs, -2) - tf.expand_dims(ys, -3)\n",
    "    # ... x Kx x Ky x D\n",
    "\n",
    "    if StrictVersion(tf.__version__) < StrictVersion('1.5.0'):\n",
    "        dist_sq = tf.reduce_sum(diff**2, axis=-1, keep_dims=False)\n",
    "    else:\n",
    "        dist_sq = tf.reduce_sum(diff**2, axis=-1, keepdims=False)\n",
    "    # ... x Kx x Ky\n",
    "\n",
    "    # Get median.\n",
    "    input_shape = tf.concat((leading_shape, [Kx * Ky]), axis=0)\n",
    "    values, _ = tf.nn.top_k(\n",
    "        input=tf.reshape(dist_sq, input_shape),\n",
    "        k=(Kx * Ky // 2 + 1),  # This is exactly true only if Kx*Ky is odd.\n",
    "        sorted=True)  # ... x floor(Ks*Kd/2)\n",
    "\n",
    "    medians_sq = values[..., -1]  # ... (shape) (last element is the median)\n",
    "\n",
    "    h = medians_sq / np.log(Kx)  # ... (shape)\n",
    "    h = tf.maximum(h, h_min)\n",
    "    h = tf.stop_gradient(h)  # Just in case.\n",
    "    h_expanded_twice = tf.expand_dims(tf.expand_dims(h, -1), -1)\n",
    "    # ... x 1 x 1\n",
    "\n",
    "    kappa = tf.exp(-dist_sq / h_expanded_twice)  # ... x Kx x Ky\n",
    "\n",
    "    # Construct the gradient\n",
    "    h_expanded_thrice = tf.expand_dims(h_expanded_twice, -1)\n",
    "    # ... x 1 x 1 x 1\n",
    "    kappa_expanded = tf.expand_dims(kappa, -1)  # ... x Kx x Ky x 1\n",
    "\n",
    "    kappa_grad = -2 * diff / h_expanded_thrice * kappa_expanded\n",
    "    # ... x Kx x Ky x D\n",
    "\n",
    "    return {\"output\": kappa, \"gradient\": kappa_grad}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def assert_shape(tensor, expected_shape):\n",
    "    tensor_shape = tensor.shape.as_list()\n",
    "    assert len(tensor_shape) == len(expected_shape)\n",
    "    assert all([a == b for a, b in zip(tensor_shape, expected_shape)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class MASQL(MARLAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_kwargs,\n",
    "            agent_id,\n",
    "            env,\n",
    "            pool,\n",
    "            qf,\n",
    "            target_qf,\n",
    "            policy,\n",
    "            plotter=None,\n",
    "            policy_lr=1E-3,\n",
    "            qf_lr=1E-3,\n",
    "            tau=0.01,\n",
    "            value_n_particles=16,\n",
    "            td_target_update_interval=1,\n",
    "            kernel_fn=adaptive_isotropic_gaussian_kernel,\n",
    "            kernel_n_particles=16,\n",
    "            kernel_update_ratio=0.5,\n",
    "            discount=0.99,\n",
    "            reward_scale=.1,\n",
    "            joint=True,\n",
    "            use_saved_qf=False,\n",
    "            use_saved_policy=False,\n",
    "            save_full_state=False,\n",
    "            train_qf=True,\n",
    "            train_policy=True,\n",
    "            joint_policy=True,\n",
    "            opponent_action_range=None,\n",
    "            opponent_action_range_normalize=True\n",
    "    ):\n",
    "        super(MASQL, self).__init__(**base_kwargs)\n",
    "\n",
    "\n",
    "        self._env = env\n",
    "        self._pool = pool\n",
    "        self.qf = qf\n",
    "        self.target_qf = target_qf\n",
    "        self._policy = policy\n",
    "        self.plotter = plotter\n",
    "\n",
    "        self.agent_id = agent_id\n",
    "\n",
    "        self._qf_lr = qf_lr\n",
    "        self._policy_lr = policy_lr\n",
    "        self._tau = tau\n",
    "        self._discount = discount\n",
    "        self._reward_scale = reward_scale\n",
    "        self.joint_policy = joint_policy\n",
    "        self.opponent_action_range = opponent_action_range\n",
    "        self.opponent_action_range_normalize = opponent_action_range_normalize\n",
    "\n",
    "        self.joint = joint\n",
    "        self._value_n_particles = value_n_particles\n",
    "        self._qf_target_update_interval = td_target_update_interval\n",
    "\n",
    "        self._kernel_fn = kernel_fn\n",
    "        self._kernel_n_particles = kernel_n_particles\n",
    "        self._kernel_update_ratio = kernel_update_ratio\n",
    "\n",
    "        self._save_full_state = save_full_state\n",
    "        self._train_qf = train_qf\n",
    "        self._train_policy = train_policy\n",
    "\n",
    "        self._observation_dim = self.env.observation_spaces[self.agent_id].flat_dim\n",
    "        self._action_dim = self.env.action_spaces[self.agent_id].flat_dim\n",
    "        # just for two agent case\n",
    "        self._opponent_action_dim = self.env.action_spaces.opponent_flat_dim(self.agent_id)\n",
    "\n",
    "        self._create_placeholders()\n",
    "\n",
    "        self._training_ops = []\n",
    "        self._target_ops = []\n",
    "\n",
    "        self._create_td_update()\n",
    "        self._create_svgd_update()\n",
    "        self._create_target_ops()\n",
    "\n",
    "        if use_saved_qf:\n",
    "            saved_qf_params = qf.get_param_values()\n",
    "        if use_saved_policy:\n",
    "            saved_policy_params = policy.get_param_values()\n",
    "\n",
    "        self._sess = tf_utils.get_default_session()\n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if use_saved_qf:\n",
    "            self.qf.set_param_values(saved_qf_params)\n",
    "        if use_saved_policy:\n",
    "            self.policy.set_param_values(saved_policy_params)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "%%add_to MASQL\n",
    "def _create_placeholders(self):\n",
    "        \"\"\"Create all necessary placeholders.\"\"\"\n",
    "\n",
    "        self._observations_ph = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=[None, self._observation_dim],\n",
    "            name='observations')\n",
    "\n",
    "        self._next_observations_ph = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=[None, self._observation_dim],\n",
    "            name='next_observations')\n",
    "\n",
    "        self._actions_pl = tf.placeholder(\n",
    "            tf.float32, shape=[None, self._action_dim],\n",
    "            name='actions_agent_{}'.format(self.agent_id))\n",
    "\n",
    "        self._opponent_actions_pl = tf.placeholder(\n",
    "            tf.float32, shape=[None, self._opponent_action_dim],\n",
    "            name='opponent_actions_agent_{}'.format(self.agent_id))\n",
    "        # self._next_actions_ph = tf.placeholder(\n",
    "        #     tf.float32, shape=[None, self._action_dim + self._opponent_action_dim],\n",
    "        #     name='next_actions_agent_{}'.format(self._agent_id))\n",
    "\n",
    "        self._rewards_pl = tf.placeholder(\n",
    "            tf.float32, shape=[None],\n",
    "            name='rewards_agent_{}'.format(self.agent_id))\n",
    "\n",
    "        self._terminals_pl = tf.placeholder(\n",
    "            tf.float32, shape=[None],\n",
    "            name='terminals_agent_{}'.format(self.agent_id))\n",
    "\n",
    "        self._annealing_pl = tf.placeholder(\n",
    "            tf.float32, shape=[],\n",
    "            name='annealing_agent_{}'.format(self.agent_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "%%add_to MASQL\n",
    "def _create_td_update(self):\n",
    "        \"\"\"Create a minimization operation for Q-function update.\"\"\"\n",
    "\n",
    "        with tf.variable_scope('target_agent_{}'.format(self.agent_id), reuse=tf.AUTO_REUSE):\n",
    "            # The value of the next state is approximated with uniform samples.\n",
    "            # target_actions = tf.random_uniform(\n",
    "            #     (1, self._value_n_particles, self._action_dim), *self._env.action_range)\n",
    "            # opponent_target_actions = tf.random_uniform(\n",
    "            #     (1, self._value_n_particles, self._opponent_action_dim), *self._env.action_range)\n",
    "\n",
    "            if self.opponent_action_range is None:\n",
    "                target_actions = tf.random_uniform(\n",
    "                    (1, self._value_n_particles, self._action_dim), *self._env.action_range)\n",
    "                opponent_target_actions = tf.random_uniform(\n",
    "                    (1, self._value_n_particles, self._opponent_action_dim), *self._env.action_range)\n",
    "            else:\n",
    "                target_actions = tf.random_uniform(\n",
    "                    (1, self._value_n_particles, self._action_dim), *self._env.action_range)\n",
    "                opponent_target_actions = tf.random_uniform(\n",
    "                    (1, self._value_n_particles, self._opponent_action_dim), *self._env.action_range)\n",
    "                if self.opponent_action_range_normalize:\n",
    "                    target_actions = tf.nn.softmax(target_actions, axis=-1)\n",
    "                    opponent_target_actions = tf.nn.softmax(opponent_target_actions, axis=-1)\n",
    "\n",
    "            target_actions = tf.concat([target_actions, opponent_target_actions], axis=2)\n",
    "\n",
    "            q_value_targets = self.target_qf.output_for(\n",
    "                observations=self._next_observations_ph[:, None, :],\n",
    "                actions=target_actions)\n",
    "\n",
    "            assert_shape(q_value_targets, [None, self._value_n_particles])\n",
    "\n",
    "        joint_action = tf.concat([self._actions_pl, self._opponent_actions_pl], axis=1)\n",
    "        self._q_values = self.qf.output_for(\n",
    "            self._observations_ph, joint_action, reuse=True)\n",
    "        assert_shape(self._q_values, [None])\n",
    "\n",
    "        # Equation 10:\n",
    "\n",
    "        next_value = self._annealing_pl * tf.reduce_logsumexp(q_value_targets / self._annealing_pl, axis=1)\n",
    "        # next_value = tf.reduce_logsumexp(q_value_targets, axis=1)\n",
    "        assert_shape(next_value, [None])\n",
    "\n",
    "\n",
    "        # Importance weights add just a constant to the value.\n",
    "        next_value -= tf.log(tf.cast(self._value_n_particles, tf.float32))\n",
    "        next_value += (self._action_dim + self._opponent_action_dim) * np.log(2)\n",
    "\n",
    "        # \\hat Q in Equation 11:\n",
    "        ys = tf.stop_gradient(self._reward_scale * self._rewards_pl + (\n",
    "            1 - self._terminals_pl) * self._discount * next_value)\n",
    "        assert_shape(ys, [None])\n",
    "\n",
    "        # Equation 11:\n",
    "        bellman_residual = 0.5 * tf.reduce_mean((ys - self._q_values)**2)\n",
    "        with tf.variable_scope('target_agent_{}'.format(self.agent_id), reuse=tf.AUTO_REUSE):\n",
    "            if self._train_qf:\n",
    "\n",
    "                td_train_op = tf.train.AdamOptimizer(self._qf_lr).minimize(\n",
    "                    loss=bellman_residual, var_list=self.qf.get_params_internal())\n",
    "                self._training_ops.append(td_train_op)\n",
    "\n",
    "        self._bellman_residual = bellman_residual"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "%%add_to MASQL\n",
    "def _create_svgd_update(self):\n",
    "        \"\"\"Create a minimization operation for policy update (SVGD).\"\"\"\n",
    "        # print('actions')\n",
    "        actions = self.policy.actions_for(\n",
    "            observations=self._observations_ph,\n",
    "            n_action_samples=self._kernel_n_particles,\n",
    "            reuse=True)\n",
    "        assert_shape(actions,\n",
    "                     [None, self._kernel_n_particles, self._action_dim + self._opponent_action_dim])\n",
    "\n",
    "        # SVGD requires computing two empirical expectations over actions\n",
    "        # (see Appendix C1.1.). To that end, we first sample a single set of\n",
    "        # actions, and later split them into two sets: `fixed_actions` are used\n",
    "        # to evaluate the expectation indexed by `j` and `updated_actions`\n",
    "        # the expectation indexed by `i`.\n",
    "        n_updated_actions = int(\n",
    "            self._kernel_n_particles * self._kernel_update_ratio)\n",
    "        n_fixed_actions = self._kernel_n_particles - n_updated_actions\n",
    "\n",
    "        fixed_actions, updated_actions = tf.split(\n",
    "            actions, [n_fixed_actions, n_updated_actions], axis=1)\n",
    "        fixed_actions = tf.stop_gradient(fixed_actions)\n",
    "        assert_shape(fixed_actions, [None, n_fixed_actions, self._action_dim + self._opponent_action_dim])\n",
    "        assert_shape(updated_actions,\n",
    "                     [None, n_updated_actions, self._action_dim + self._opponent_action_dim])\n",
    "   \n",
    "        svgd_target_values = self.qf.output_for(\n",
    "            self._observations_ph[:, None, :], fixed_actions, reuse=True) / self._annealing_pl\n",
    "\n",
    "        # Target log-density. Q_soft in Equation 13:\n",
    "        squash_correction = tf.reduce_sum(\n",
    "            tf.log(1 - fixed_actions**2 + EPS), axis=-1)\n",
    "        log_p = svgd_target_values + squash_correction\n",
    "\n",
    "        grad_log_p = tf.gradients(log_p, fixed_actions)[0]\n",
    "        grad_log_p = tf.expand_dims(grad_log_p, axis=2)\n",
    "        grad_log_p = tf.stop_gradient(grad_log_p)\n",
    "        assert_shape(grad_log_p, [None, n_fixed_actions, 1, self._action_dim + self._opponent_action_dim])\n",
    "\n",
    "        kernel_dict = self._kernel_fn(xs=fixed_actions, ys=updated_actions)\n",
    "\n",
    "        # Kernel function in Equation 13:\n",
    "        kappa = tf.expand_dims(kernel_dict[\"output\"], dim=3)\n",
    "        assert_shape(kappa, [None, n_fixed_actions, n_updated_actions, 1])\n",
    "\n",
    "        # Stein Variational Gradient in Equation 13:\n",
    "        action_gradients = tf.reduce_mean(\n",
    "            kappa * grad_log_p + kernel_dict[\"gradient\"], reduction_indices=1)\n",
    "        assert_shape(action_gradients,\n",
    "                     [None, n_updated_actions, self._action_dim + self._opponent_action_dim])\n",
    "\n",
    "        # Propagate the gradient through the policy network (Equation 14).\n",
    "        gradients = tf.gradients(\n",
    "            updated_actions,\n",
    "            self.policy.get_params_internal(),\n",
    "            grad_ys=action_gradients)\n",
    "\n",
    "        surrogate_loss = tf.reduce_sum([\n",
    "            tf.reduce_sum(w * tf.stop_gradient(g))\n",
    "            for w, g in zip(self.policy.get_params_internal(), gradients)\n",
    "        ])\n",
    "        with tf.variable_scope('policy_opt_agent_{}'.format(self.agent_id), reuse=tf.AUTO_REUSE):\n",
    "            if self._train_policy:\n",
    "                optimizer = tf.train.AdamOptimizer(self._policy_lr)\n",
    "                svgd_training_op = optimizer.minimize(\n",
    "                    loss=-surrogate_loss,\n",
    "                    var_list=self.policy.get_params_internal())\n",
    "                self._training_ops.append(svgd_training_op)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "%%add_to MASQL\n",
    "def _create_target_ops(self):\n",
    "        \"\"\"Create tensorflow operation for updating the target Q-function.\"\"\"\n",
    "        if not self._train_qf:\n",
    "            return\n",
    "\n",
    "        source_params = self.qf.get_params_internal()\n",
    "        target_params = self.target_qf.get_params_internal()\n",
    "\n",
    "        self._target_ops = [\n",
    "            tf.assign(target, (1 - self._tau) * target + self._tau * source)\n",
    "            for target, source in zip(target_params, source_params)\n",
    "        ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "%%add_to MASQL\n",
    "def train(self):\n",
    "        self._train(self.env, self.policy, self.pool)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "%%add_to MASQL\n",
    "@overrides\n",
    "def _init_training(self):\n",
    "    self._sess.run(self._target_ops)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%%add_to MASQL\n",
    "@overrides\n",
    "def _do_training(self, iteration, batch, annealing=1.):\n",
    "    \"\"\"Run the operations for updating training and target ops.\"\"\"\n",
    "    self.log_diagnostics (batch,annealing)\n",
    "    feed_dict = self._get_feed_dict(batch, annealing)\n",
    "    self._sess.run(self._training_ops, feed_dict)\n",
    "    if iteration % self._qf_target_update_interval == 0 and self._train_qf:\n",
    "        self._sess.run(self._target_ops)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "%%add_to MASQL\n",
    "def _get_feed_dict(self, batch, annealing):\n",
    "    \"\"\"Construct a TensorFlow feed dictionary from a sample batch.\"\"\"\n",
    "\n",
    "    feeds = {\n",
    "            self._observations_ph: batch['observations'],\n",
    "            self._actions_pl: batch['actions'],\n",
    "            self._opponent_actions_pl: batch['opponent_actions'],\n",
    "            self._next_observations_ph: batch['next_observations'],\n",
    "            self._rewards_pl: batch['rewards'],\n",
    "            self._terminals_pl: batch['terminals'],\n",
    "            self._annealing_pl: annealing\n",
    "        }\n",
    "\n",
    "    return feeds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%%add_to MASQL\n",
    "@overrides\n",
    "def log_diagnostics(self, batch,annealing):\n",
    "        \"\"\"Record diagnostic information.\n",
    "        Records the mean and standard deviation of Q-function and the\n",
    "        squared Bellman residual of the  s (mean squared Bellman error)\n",
    "        for a sample batch.\n",
    "        Also call the `draw` method of the plotter, if plotter is defined.\n",
    "        \"\"\"\n",
    "\n",
    "        feeds = self._get_feed_dict(batch)\n",
    "        qf, bellman_residual = self._sess.run(\n",
    "            [self._q_values, self._bellman_residual], feeds)\n",
    "  \n",
    "\n",
    "        logger.record_tabular('qf-avg-agent-{}'.format(self.agent_id), np.mean(qf))\n",
    "        logger.record_tabular('qf-std-agent-{}'.format(self.agent_id), np.std(qf))\n",
    "        logger.record_tabular('mean-sq-bellman-error-agent-{}'.format(self.agent_id), bellman_residual)\n",
    "\n",
    "        self.policy.log_diagnostics(batch)\n",
    "        # if self.plotter:\n",
    "        #     self.plotter.draw()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "%%add_to MASQL\n",
    "@overrides\n",
    "def get_snapshot(self, epoch):\n",
    "        \"\"\"Return loggable snapshot of the SQL algorithm.\n",
    "        If `self._save_full_state == True`, returns snapshot including the\n",
    "        replay buffer. If `self._save_full_state == False`, returns snapshot\n",
    "        of policy, Q-function, and environment instances.\n",
    "        \"\"\"\n",
    "\n",
    "        state = {\n",
    "            'epoch_agent_{}'.format(self.agent_id): epoch,\n",
    "            'policy_agent_{}'.format(self.agent_id): self.policy,\n",
    "            'qf_agent_{}'.format(self.agent_id): self.qf,\n",
    "            'env_agent_{}'.format(self.agent_id): self.env,\n",
    "        }\n",
    "\n",
    "        if self._save_full_state:\n",
    "            state.update({'replay_buffer_agent_{}'.format(self.agent_id): self.pool})\n",
    "\n",
    "        return state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from maci.replay_buffers import SimpleReplayBuffer\n",
    "from maci.value_functions.sq_value_function import NNQFunction\n",
    "from maci.policies import  StochasticNNPolicy\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def masql_agent(model_name, i, env, M, u_range, base_kwargs, game_name='diff-ma_softq'):\n",
    "    joint = True\n",
    "    squash = True\n",
    "    squash_func = tf.tanh\n",
    "    sampling = False\n",
    "\n",
    "    pool = SimpleReplayBuffer(env.env_specs, max_replay_buffer_size=1e4, joint=joint, agent_id=i)\n",
    "    policy = StochasticNNPolicy(env.env_specs,\n",
    "                                hidden_layer_sizes=(M, M),\n",
    "                                squash=squash, squash_func=squash_func, sampling=sampling, u_range=u_range, joint=joint,\n",
    "                                agent_id=i)\n",
    "\n",
    "    qf = NNQFunction(env_spec=env.env_specs, hidden_layer_sizes=[M, M], joint=joint, agent_id=i)\n",
    "    target_qf = NNQFunction(env_spec=env.env_specs, hidden_layer_sizes=[M, M], name='target_qf', joint=joint,\n",
    "                            agent_id=i)\n",
    "  \n",
    "    plotter = None\n",
    "\n",
    "    agent = MASQL(\n",
    "        base_kwargs=base_kwargs,\n",
    "        agent_id=i,\n",
    "        env=env,\n",
    "        pool=pool,\n",
    "        qf=qf,\n",
    "        target_qf=target_qf,\n",
    "        policy=policy,\n",
    "        plotter=plotter,\n",
    "        policy_lr=3e-4,\n",
    "        qf_lr=3e-4,\n",
    "        tau=0.01,\n",
    "        value_n_particles=16,\n",
    "        td_target_update_interval=10,\n",
    "        kernel_fn=adaptive_isotropic_gaussian_kernel,\n",
    "        kernel_n_particles=32,\n",
    "        kernel_update_ratio=0.5,\n",
    "        discount=0.99,\n",
    "        reward_scale=1,\n",
    "        save_full_state=False)\n",
    "    return agent\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "from maci.misc.sampler import MASampler\n",
    "from maci.environments import  DifferentialGame\n",
    "from maci.misc import logger\n",
    "import gtimer as gt\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import maci.misc.tf_utils as U\n",
    "import os\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "parser = argparse.ArgumentParser(\"Reinforcement Learning experiments for multiagent environments\")\n",
    "# Environment\n",
    "# ['particle-simple_spread', 'particle-simple_adversary', 'particle-simple_tag', 'particle-simple_push']\n",
    "# matrix-prison , matrix-prison\n",
    "# pbeauty\n",
    "parser.add_argument('-g', \"--game_name\", type=str, default=\"diff-ma_softq\", help=\"name of the game\")\n",
    "parser.add_argument('-p', \"--p\", type=float, default=1.1, help=\"p\")\n",
    "parser.add_argument('-mu', \"--mu\", type=float, default=1.5, help=\"mu\")\n",
    "parser.add_argument('-r', \"--reward_type\", type=str, default=\"abs\", help=\"reward type\")\n",
    "parser.add_argument('-mp', \"--max_path_length\", type=int, default=1, help=\"reward type\")\n",
    "parser.add_argument('-ms', \"--max_steps\", type=int, default=50, help=\"reward type\")\n",
    "parser.add_argument('-me', \"--memory\", type=int, default=0, help=\"reward type\")\n",
    "parser.add_argument('-n', \"--n\", type=int, default=2, help=\"name of the game\")\n",
    "parser.add_argument('-bs', \"--batch_size\", type=int, default=5, help=\"name of the game\")\n",
    "parser.add_argument('-hm', \"--hidden_size\", type=int, default=100, help=\"name of the game\")\n",
    "parser.add_argument('-re', \"--repeat\", type=bool, default=False, help=\"name of the game\")\n",
    "parser.add_argument('-a', \"--aux\", type=bool, default=True, help=\"name of the game\")\n",
    "parser.add_argument('-m', \"--model_names_setting\", type=str, default='MASQL_MASQL', help=\"models setting agent vs adv\")\n",
    "arglist, unknown = parser.parse_known_args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "\n",
    "game_name = arglist.game_name\n",
    "# 'abs', 'one'\n",
    "reward_type = arglist.reward_type\n",
    "p = arglist.p\n",
    "agent_num = arglist.n\n",
    "u_range = 1.\n",
    "k = 0\n",
    "model_names_setting = arglist.model_names_setting.split('_')\n",
    "model_names = [model_names_setting[0]] + [model_names_setting[1]] * (agent_num - 1)\n",
    "model_name = '_'.join(model_names)\n",
    "path_prefix = game_name"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "diff_game_name = game_name.split('-')[-1]\n",
    "agent_num = 2\n",
    "env = DifferentialGame(diff_game_name, agent_num)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime('%Y-%m-%d %H:%M:%S.%f %Z')\n",
    "if 'CG' in model_name:\n",
    "    model_name = model_name + '-{}'.format(arglist.mu)\n",
    "if not arglist.aux:\n",
    "    model_name = model_name + '-{}'.format(arglist.aux)\n",
    "\n",
    "suffix = '{}/{}/{}/{}'.format(path_prefix, agent_num, model_name, timestamp)\n",
    "\n",
    "print(suffix)\n",
    "\n",
    "\n",
    "logger.add_tabular_output('./log/{}.csv'.format(suffix))\n",
    "snapshot_dir = './snapshot/{}'.format(suffix)\n",
    "policy_dir = './policy/{}'.format(suffix)\n",
    "os.makedirs(snapshot_dir, exist_ok=True)\n",
    "os.makedirs(policy_dir, exist_ok=True)\n",
    "logger.set_snapshot_dir(snapshot_dir)\n",
    "\n",
    "agents = []\n",
    "M = arglist.hidden_size\n",
    "batch_size = arglist.batch_size\n",
    "sampler = MASampler(agent_num=agent_num, joint=True, max_path_length=30, min_pool_size=1, batch_size=batch_size)\n",
    "\n",
    "base_kwargs = {\n",
    "    'sampler': sampler,\n",
    "    'epoch_length': 1,\n",
    "    'n_epochs': arglist.max_steps,\n",
    "    'n_train_repeat': 5,\n",
    "    'eval_render': True,\n",
    "    'eval_n_episodes': 10\n",
    "}\n",
    "agents"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "diff-ma_softq/2/MASQL_MASQL/2021-07-16 15:09:06.952409 \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "with U.single_threaded_session():\n",
    "    for i, model_name in enumerate(model_names):\n",
    "\n",
    "        agent = masql_agent(model_name, i, env, M, u_range, base_kwargs, game_name=game_name)\n",
    "        agents.append(agent)\n",
    "    sampler.initialize(env, agents)\n",
    "\n",
    "    for agent in agents:\n",
    "        agent._init_training()\n",
    "        gt.rename_root('MARLAlgorithm')\n",
    "        gt.reset()\n",
    "        gt.set_def_unique(False)\n",
    "        initial_exploration_done = False\n",
    "        # noise = .1\n",
    "        noise = 1.\n",
    "        alpha = .5\n",
    "\n",
    "\n",
    "        for agent in agents:\n",
    "            try:\n",
    "                agent.policy.set_noise_level(noise)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for epoch in gt.timed_for(range(base_kwargs['n_epochs'] + 1)):\n",
    "            logger.push_prefix('Epoch #%d | ' % epoch)\n",
    "            if epoch % 1 == 0:\n",
    "                print(suffix)\n",
    "            for t in range(base_kwargs['epoch_length']):\n",
    "                # TODO.code consolidation: Add control interval to sampler\n",
    "                if not initial_exploration_done:\n",
    "                    if epoch >= 3:\n",
    "                        initial_exploration_done = True\n",
    "                sampler.sample()\n",
    "                # print('Sampling')\n",
    "                if not initial_exploration_done:\n",
    "                    continue\n",
    "                gt.stamp('sample')\n",
    "                # print('Sample Done')\n",
    "                if epoch == base_kwargs['n_epochs']:\n",
    "                    noise = 0.1\n",
    "\n",
    "                    for agent in agents:\n",
    "                        try:\n",
    "                            agent.policy.set_noise_level(noise)\n",
    "                        except:\n",
    "                            pass\n",
    "                    # alpha = .1\n",
    "                if epoch > base_kwargs['n_epochs'] / 10:\n",
    "                    noise = 0.1\n",
    "                    for agent in agents:\n",
    "                        try:\n",
    "                            agent.policy.set_noise_level(noise)\n",
    "                        except:\n",
    "                            pass\n",
    "                    # alpha = .1\n",
    "                if epoch > base_kwargs['n_epochs'] / 5:\n",
    "                    noise = 0.05\n",
    "                    for agent in agents:\n",
    "                        try:\n",
    "                            agent.policy.set_noise_level(noise)\n",
    "                        except:\n",
    "                            pass\n",
    "                if epoch > base_kwargs['n_epochs'] / 6:\n",
    "                    noise = 0.01\n",
    "                    for agent in agents:\n",
    "                        try:\n",
    "                            agent.policy.set_noise_level(noise)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                for j in range(base_kwargs['n_train_repeat']):\n",
    "                    batch_n = []\n",
    "                    recent_batch_n = []\n",
    "                    indices = None\n",
    "                    receent_indices = None\n",
    "                    for i, agent in enumerate(agents):\n",
    "                        if i == 0:\n",
    "                            batch = agent.pool.random_batch(batch_size)\n",
    "                            indices = agent.pool.indices\n",
    "                            receent_indices = list(range(agent.pool._top-batch_size, agent.pool._top))\n",
    "\n",
    "                        batch_n.append(agent.pool.random_batch_by_indices(indices))\n",
    "                        recent_batch_n.append(agent.pool.random_batch_by_indices(receent_indices))\n",
    "\n",
    "                    # print(len(batch_n))\n",
    "                    target_next_actions_n = []\n",
    "                    try:\n",
    "                        for agent, batch in zip(agents, batch_n):\n",
    "                            target_next_actions_n.append(agent._target_policy.get_actions(batch['next_observations']))\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                    opponent_actions_n = np.array([batch['actions'] for batch in batch_n])\n",
    "                    recent_opponent_actions_n = np.array([batch['actions'] for batch in recent_batch_n])\n",
    "\n",
    "                    ####### figure out\n",
    "                    recent_opponent_observations_n = []\n",
    "                    for batch in recent_batch_n:\n",
    "                        recent_opponent_observations_n.append(batch['observations'])\n",
    "\n",
    "\n",
    "                    current_actions = [agents[i]._policy.get_actions(batch_n[i]['next_observations'])[0][0] for i in range(agent_num)]\n",
    "                   \n",
    "                    with open('{}/policy.csv'.format(policy_dir), 'a') as f:\n",
    "                        f.write(','.join(list(map(str, current_actions)))+'\\n')\n",
    "                    for i, agent in enumerate(agents):\n",
    "                        try:\n",
    "                            batch_n[i]['next_actions'] = deepcopy(target_next_actions_n[i])\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                        batch_n[i]['opponent_actions'] = np.reshape(np.delete(deepcopy(opponent_actions_n), i, 0), (-1, agent._opponent_action_dim))\n",
    "                      \n",
    "                        if agent.joint:\n",
    "                         \n",
    "                            batch_n[i]['opponent_next_actions'] = 11 ##np.reshape(np.delete(deepcopy(target_next_actions_n), i, 0), (-1, agent._opponent_action_dim))\n",
    "                         \n",
    "                            agent._do_training(iteration=t + epoch * agent._epoch_length, batch=batch_n[i], annealing=alpha)\n",
    "                 \n",
    "                gt.stamp('train')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 MASQL\n",
      "Build agents!!!!!!!!!!!!!!!!\n",
      "0 1\n",
      "stochastic 1.0 True <function tanh at 0x7f4a8673ad90>\n",
      "stochastic 1.0 True <function tanh at 0x7f4a8673ad90>\n",
      "WARNING:tensorflow:From <string>:43: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "1 MASQL\n",
      "Build agents!!!!!!!!!!!!!!!!\n",
      "1 1\n",
      "stochastic 1.0 True <function tanh at 0x7f4a8673ad90>\n",
      "stochastic 1.0 True <function tanh at 0x7f4a8673ad90>\n",
      "diff-ma_softq/2/MASQL_MASQL/2021-07-16 15:09:06.952409 \n",
      "[-13.06029034 -13.06029034]\n",
      "2021-07-16 15:09:10.088235 SAST | ------------------------  --------\n",
      "2021-07-16 15:09:10.088836 SAST | max-path-return_agent_0   -13.0603\n",
      "2021-07-16 15:09:10.089181 SAST | mean-path-return_agent_0  -13.0603\n",
      "2021-07-16 15:09:10.089726 SAST | last-path-return_agent_0  -13.0603\n",
      "2021-07-16 15:09:10.090114 SAST | max-path-return_agent_1   -13.0603\n",
      "2021-07-16 15:09:10.090504 SAST | mean-path-return_agent_1  -13.0603\n",
      "2021-07-16 15:09:10.090992 SAST | last-path-return_agent_1  -13.0603\n",
      "2021-07-16 15:09:10.092724 SAST | episodes                    1\n",
      "2021-07-16 15:09:10.093260 SAST | total-samples               1\n",
      "2021-07-16 15:09:10.094112 SAST | ------------------------  --------\n",
      "diff-ma_softq/2/MASQL_MASQL/2021-07-16 15:09:06.952409 \n",
      "[-13.65899563 -13.65899563]\n",
      "2021-07-16 15:09:10.107179 SAST | ------------------------  --------\n",
      "2021-07-16 15:09:10.108206 SAST | max-path-return_agent_0   -13.0603\n",
      "2021-07-16 15:09:10.108743 SAST | mean-path-return_agent_0  -13.659\n",
      "2021-07-16 15:09:10.109239 SAST | last-path-return_agent_0  -13.659\n",
      "2021-07-16 15:09:10.109704 SAST | max-path-return_agent_1   -13.0603\n",
      "2021-07-16 15:09:10.110147 SAST | mean-path-return_agent_1  -13.659\n",
      "2021-07-16 15:09:10.110697 SAST | last-path-return_agent_1  -13.659\n",
      "2021-07-16 15:09:10.112710 SAST | episodes                    2\n",
      "2021-07-16 15:09:10.113294 SAST | total-samples               2\n",
      "2021-07-16 15:09:10.113779 SAST | ------------------------  --------\n",
      "diff-ma_softq/2/MASQL_MASQL/2021-07-16 15:09:06.952409 \n",
      "[-12.8417654 -12.8417654]\n",
      "2021-07-16 15:09:10.127273 SAST | ------------------------  --------\n",
      "2021-07-16 15:09:10.128091 SAST | max-path-return_agent_0   -12.8418\n",
      "2021-07-16 15:09:10.128839 SAST | mean-path-return_agent_0  -12.8418\n",
      "2021-07-16 15:09:10.129492 SAST | last-path-return_agent_0  -12.8418\n",
      "2021-07-16 15:09:10.130257 SAST | max-path-return_agent_1   -12.8418\n",
      "2021-07-16 15:09:10.130611 SAST | mean-path-return_agent_1  -12.8418\n",
      "2021-07-16 15:09:10.130925 SAST | last-path-return_agent_1  -12.8418\n",
      "2021-07-16 15:09:10.131240 SAST | episodes                    3\n",
      "2021-07-16 15:09:10.131541 SAST | total-samples               3\n",
      "2021-07-16 15:09:10.131897 SAST | ------------------------  --------\n",
      "diff-ma_softq/2/MASQL_MASQL/2021-07-16 15:09:06.952409 \n",
      "[-13.72172737 -13.72172737]\n",
      "2021-07-16 15:09:10.151545 SAST | ------------------------  --------\n",
      "2021-07-16 15:09:10.152184 SAST | max-path-return_agent_0   -12.8418\n",
      "2021-07-16 15:09:10.152637 SAST | mean-path-return_agent_0  -13.7217\n",
      "2021-07-16 15:09:10.152992 SAST | last-path-return_agent_0  -13.7217\n",
      "2021-07-16 15:09:10.153311 SAST | max-path-return_agent_1   -12.8418\n",
      "2021-07-16 15:09:10.153603 SAST | mean-path-return_agent_1  -13.7217\n",
      "2021-07-16 15:09:10.153871 SAST | last-path-return_agent_1  -13.7217\n",
      "2021-07-16 15:09:10.154158 SAST | episodes                    4\n",
      "2021-07-16 15:09:10.154434 SAST | total-samples               4\n",
      "2021-07-16 15:09:10.154732 SAST | ------------------------  --------\n",
      "Checking agent.joint, which should be true True\n",
      "[] 0\n",
      "DO WE EVER GET HERE\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "_get_feed_dict() missing 1 required positional argument: 'annealing'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-422dc9a03a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m                             \u001b[0mbatch_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opponent_next_actions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m \u001b[0;31m##np.reshape(np.delete(deepcopy(target_next_actions_n), i, 0), (-1, agent._opponent_action_dim))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DO WE EVER GET HERE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannealing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m_do_training\u001b[0;34m(self, iteration, batch, annealing)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mlog_diagnostics\u001b[0;34m(self, batch, annealing)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _get_feed_dict() missing 1 required positional argument: 'annealing'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}